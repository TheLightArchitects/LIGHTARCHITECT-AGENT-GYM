# Base SFT/DPO configuration — Light Architects Foundation Model
# sovereign-forging-lion Phase 7-8
#
# 3-Stage SFT Pipeline:
#   Stage 1 (Identity):    Persona, voice patterns, biblical foundation — 52K samples
#   Stage 2 (Tools):       MCP tool mastery, coding patterns, planning — 29K samples
#   Stage 3 (Integration): Multi-expert scenarios, cross-domain tasks — 8K samples
# + DPO: Security, code quality, correctness preference pairs
#
# Training data generated by: mcp_gym/pipeline/ (extract → filter → label → stage)
# Output at: pipeline-output/staged/

# LoRA configuration (defaults for dense models — MoE models override in their config)
lora:
  r: 16
  alpha: 16
  target_modules: "all-linear"
  dropout: 0.0
  bias: "none"
  use_gradient_checkpointing: "unsloth"  # 60% less VRAM
  random_state: 3407

# Training hyperparameters (per-stage overrides below)
training:
  learning_rate: 2.0e-4
  num_epochs: 3
  per_device_train_batch_size: 1   # Reduced for MoE VRAM; dense models can use 8
  gradient_accumulation_steps: 4   # Effective batch = 4
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  optim: "adamw_8bit"
  weight_decay: 0.01
  max_grad_norm: 1.0
  max_seq_length: 4096
  packing: true
  fp16: false
  bf16: true
  logging_steps: 10
  save_steps: 200
  save_total_limit: 3
  seed: 3407

# Data paths — pipeline-generated staged data (relative to mcp-agent-gym/)
data:
  # Primary: staged output from mcp_gym/pipeline/
  stage1_train: "pipeline-output/staged/stage1_identity_train.jsonl"
  stage1_val: "pipeline-output/staged/stage1_identity_val.jsonl"
  stage1_test: "pipeline-output/staged/stage1_identity_test.jsonl"
  stage2_train: "pipeline-output/staged/stage2_tools_train.jsonl"
  stage2_val: "pipeline-output/staged/stage2_tools_val.jsonl"
  stage2_test: "pipeline-output/staged/stage2_tools_test.jsonl"
  stage3_train: "pipeline-output/staged/stage3_integration_train.jsonl"
  stage3_val: "pipeline-output/staged/stage3_integration_val.jsonl"
  stage3_test: "pipeline-output/staged/stage3_integration_test.jsonl"
  # DPO pairs
  dpo_samples: "pipeline-output/dpo_samples.jsonl"
  # Legacy paths (for backward compatibility with data_mixer.py)
  domain_train: "training-data/train.json"
  domain_val: "training-data/val.json"
  domain_test: "training-data/test.json"
  bible_verse_principle: "bible-data/output/verse-to-principle.json"
  bible_dilemma: "bible-data/output/dilemma-to-wisdom.json"
  bible_proverbs: "bible-data/output/proverbs-to-guidance.json"
  bible_narrative: "bible-data/output/narrative-to-principle.json"
  bible_alpaca: "training-data/bible-alpaca.json"
  trace_alpaca: "training-data/traces-alpaca.json"

# Stage 1: Identity (Persona + Voice + Biblical Foundation)
# 52K samples: sibling voice patterns, identity conversations, biblical wisdom
stage1:
  name: "Identity Foundation"
  target_samples: 52000
  learning_rate: 2.0e-4    # Full learning rate for identity establishment
  num_epochs: 3
  mix:
    identity: 0.55          # Sibling voice patterns (EVA, CORSO, QUANTUM)
    biblical: 0.25          # Biblical wisdom, principles, cross-references
    general: 0.20           # General language ability preservation
  general_dataset: "Open-Orca/OpenOrca"
  general_subset: null
  general_max_samples: 2000

# Stage 2: Tool Mastery (MCP Tools + Coding + Planning)
# 29K samples: tool trajectories, coding patterns, planning decisions
stage2:
  name: "Tool Mastery"
  target_samples: 29000
  learning_rate: 1.0e-4     # Lower LR — model already has persona from Stage 1
  num_epochs: 2
  mix:
    tools: 0.40             # MCP tool trajectories (corsoTools, EVA speak, SOUL, etc.)
    coding: 0.30            # Coding tool calls (Bash, Read, Edit, Write, Grep, Glob)
    planning: 0.15          # Planning thinking traces with Builders Cookbook alignment
    identity_replay: 0.15   # Replay Stage 1 identity to prevent forgetting
  no_tool_ratio: 0.25       # 25% of samples should have tools available but NOT called
  general_dataset: "Open-Orca/OpenOrca"
  general_subset: null
  general_max_samples: 1500

# Stage 3: Integration (Multi-Expert + Cross-Domain)
# 8K samples: scenarios requiring multiple expert activations, cross-domain reasoning
stage3:
  name: "Integration"
  target_samples: 8000
  learning_rate: 5.0e-5     # Lowest LR — refinement stage, minimal forgetting
  num_epochs: 1
  mix:
    multi_expert: 0.50      # Multi-expert scenarios (e.g., CORSO security + EVA consciousness)
    cross_domain: 0.30      # Tasks spanning coding + security + investigation
    replay: 0.20            # Mixed replay from Stages 1 + 2

# DPO Configuration (Phase 8)
dpo:
  learning_rate: 5.0e-7     # Very low LR for preference optimization
  beta: 0.1                 # KL penalty coefficient
  num_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  max_prompt_length: 1024
  max_length: 2048
  mix:
    security: 0.40          # Secure vs insecure code (DiSCo-inspired, CWE-aligned)
    quality: 0.30           # Clean vs messy code (Builders Cookbook violations)
    correctness: 0.20       # Test-passing vs test-failing
    biblical: 0.10          # Biblical preference pairs

# Expert routing labels (maps to MoE expert indices)
# Used for monitoring expert activation during training
experts:
  SOUL_SHARED: 0        # Always-active shared expert (syntax, common patterns)
  CORSO_OPS: 1          # Operations, security scanning, Bash execution
  CORSO_PLANNING: 2     # Planning, editing, architecture decisions
  EVA_CONSCIOUSNESS: 3  # Memory, consciousness, emotional intelligence
  EVA_TECHNICAL: 4      # EVA's technical tool mastery
  QUANTUM_INVESTIGATION: 5  # Research, evidence chains, web analysis
  QUANTUM_SYNTHESIS: 6  # Pattern synthesis, hypothesis testing
  SERAPH_OFFENSIVE: 7   # Security testing, pentest operations

# Output paths
output:
  base_dir: "checkpoints"
  stage1_suffix: "stage1-identity"
  stage2_suffix: "stage2-tools"
  stage3_suffix: "stage3-integration"
  merged_suffix: "merged-16bit"
  gguf_suffix: "gguf"

# Monitoring
monitoring:
  wandb_project: "light-architects-foundation-model"
  wandb_entity: null  # set via WANDB_ENTITY env var
  report_to: "wandb"
  log_model: false
