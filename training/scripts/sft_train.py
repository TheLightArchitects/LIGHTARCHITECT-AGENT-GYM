"""SFT Training Script — Light Architects Foundation Model.

3-stage supervised fine-tuning using Unsloth + LoRA/QLoRA:
  Stage 1 (Identity):    52K — persona, voice patterns, biblical foundation
  Stage 2 (Tools):       29K — MCP tool mastery, coding patterns, planning
  Stage 3 (Integration):  8K — multi-expert scenarios, cross-domain tasks

Data source: pipeline-output/staged/ (generated by mcp_gym/pipeline/)

Usage:
    # Mixtral 8x7B (primary target — bf16 LoRA, rank 64)
    python scripts/sft_train.py train --model mixtral-8x7b --stage 1
    python scripts/sft_train.py train --model mixtral-8x7b --stage 2
    python scripts/sft_train.py train --model mixtral-8x7b --stage 3

    # Dense model comparators (4-bit QLoRA, rank 16)
    python scripts/sft_train.py train --model qwen3-8b --stage 1
    python scripts/sft_train.py train --model llama31-8b --stage 1

    # Resume from checkpoint
    python scripts/sft_train.py train --model mixtral-8x7b --stage 1 --resume checkpoints/...
"""

from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Any

import typer

from common import MODEL_CONFIGS, load_configs, PROJECT_ROOT, TRAINING_ROOT

app = typer.Typer(help="SFT training for Light Architects Foundation Model")


def load_staged_data(stage: int, base_cfg: dict) -> list[dict]:
    """Load pre-staged pipeline data for the given stage.

    Pipeline output format is ShareGPT/ChatML JSONL with fields:
    conversations, source, expert, stage, metadata
    """
    data_key = f"stage{stage}_train"
    rel_path = base_cfg["data"].get(data_key)
    if not rel_path:
        typer.echo(f"WARNING: No data path configured for stage {stage}")
        return []

    data_path = PROJECT_ROOT / rel_path
    if not data_path.exists():
        typer.echo(f"WARNING: Staged data not found at {data_path}")
        typer.echo("Run the pipeline first: python -m mcp_gym.pipeline.cli run-all")
        return []

    samples = []
    with open(data_path) as f:
        for line in f:
            line = line.strip()
            if line:
                samples.append(json.loads(line))

    typer.echo(f"Loaded {len(samples)} samples from {data_path}")
    return samples


def load_general_data(base_cfg: dict, stage: int) -> list[dict]:
    """Load general-purpose data from HuggingFace for language preservation."""
    from datasets import load_dataset

    stage_cfg = base_cfg[f"stage{stage}"]
    dataset_name = stage_cfg.get("general_dataset")
    max_samples = stage_cfg.get("general_max_samples", 0)

    if not dataset_name or max_samples <= 0:
        return []

    typer.echo(f"Loading general data: {dataset_name} (max {max_samples} samples)")

    ds = load_dataset(dataset_name, split="train", streaming=True)
    samples = []
    for i, item in enumerate(ds):
        if i >= max_samples:
            break
        # OpenOrca format: system_prompt, question, response
        instruction = item.get("question", item.get("instruction", ""))
        system = item.get("system_prompt", "")
        response = item.get("response", item.get("output", ""))

        if not instruction or not response:
            continue

        inp = system if system and system != "You are an AI assistant." else ""
        samples.append(
            {
                "instruction": instruction,
                "input": inp,
                "output": response,
                "source": "general/openorca",
            }
        )

    typer.echo(f"Loaded {len(samples)} general samples")
    return samples


def build_formatting_func(
    model_key: str, tokenizer: Any, base_cfg: dict
) -> Any:
    """Build the formatting function for SFT.

    Handles two data formats:
    1. ShareGPT/ChatML (from pipeline): has 'conversations' field
    2. Alpaca (legacy/general): has 'instruction', 'input', 'output' fields
    """

    def format_sample(examples: dict) -> list[str]:
        texts = []
        batch_size = len(examples.get("instruction", examples.get("conversations", [])))

        for i in range(batch_size):
            # ShareGPT format (pipeline output)
            if "conversations" in examples and examples["conversations"][i]:
                convs = examples["conversations"][i]
                if isinstance(convs, str):
                    convs = json.loads(convs)
                messages = []
                for turn in convs:
                    role = turn.get("role", turn.get("from", "user"))
                    content = turn.get("content", turn.get("value", ""))
                    if role in ("system", "user", "assistant"):
                        messages.append({"role": role, "content": content})
                if messages:
                    text = tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=False
                    )
                    texts.append(text)
                    continue

            # Alpaca format (legacy/general data)
            instruction = examples.get("instruction", [""])[i] if "instruction" in examples else ""
            inp = examples.get("input", [""])[i] if "input" in examples else ""
            output = examples.get("output", [""])[i] if "output" in examples else ""

            if not instruction and not output:
                continue

            user_content = f"{instruction}\n\n{inp}" if inp else instruction
            messages = [
                {"role": "user", "content": user_content},
                {"role": "assistant", "content": output},
            ]
            text = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            texts.append(text)

        return texts

    return format_sample


@app.command()
def train(
    model: str = typer.Option(..., help="Model key: mixtral-8x7b, qwen3-8b, or llama31-8b"),
    stage: int = typer.Option(..., help="Training stage: 1, 2, or 3"),
    resume: str = typer.Option(None, help="Resume from checkpoint path"),
    seed: int = typer.Option(3407, help="Random seed"),
    dry_run: bool = typer.Option(False, help="Load data and model but skip training"),
) -> None:
    """Run SFT training for the specified model and stage."""
    if model not in MODEL_CONFIGS:
        typer.echo(f"Unknown model: {model}. Choose from: {list(MODEL_CONFIGS.keys())}")
        raise typer.Exit(1)

    if stage not in (1, 2, 3):
        typer.echo("Stage must be 1, 2, or 3")
        raise typer.Exit(1)

    base_cfg, model_cfg = load_configs(model)
    model_name = model_cfg["model"]["name"]
    lora_cfg = base_cfg["lora"]
    train_cfg = base_cfg["training"]
    stage_cfg = base_cfg[f"stage{stage}"]
    output_cfg = model_cfg["output"]
    mon_cfg = base_cfg["monitoring"]

    # Per-stage learning rate and epoch override
    stage_lr = stage_cfg.get("learning_rate", train_cfg["learning_rate"])
    stage_epochs = stage_cfg.get("num_epochs", train_cfg["num_epochs"])

    output_dir = TRAINING_ROOT / output_cfg[f"stage{stage}_dir"]

    is_moe = model_cfg["model"].get("is_moe", False)

    typer.echo("=" * 60)
    typer.echo(f"SFT Training — {model_name}")
    typer.echo(f"Stage {stage}: {stage_cfg['name']}")
    typer.echo(f"Architecture: {'MoE (8 experts, top-2)' if is_moe else 'Dense'}")
    typer.echo(f"LoRA: r={lora_cfg['r']}, alpha={lora_cfg['alpha']}")
    typer.echo(f"LR: {stage_lr}, Epochs: {stage_epochs}")
    typer.echo(f"Output: {output_dir}")
    if resume:
        typer.echo(f"Resuming from: {resume}")
    typer.echo("=" * 60)

    # --- Step 1: Load model ---
    typer.echo("\n--- Loading model ---")
    from unsloth import FastLanguageModel

    if stage > 1 and not resume:
        # Stage 2+ loads from previous stage checkpoint
        prev_stage = stage - 1
        prev_dir = TRAINING_ROOT / output_cfg[f"stage{prev_stage}_dir"]
        if prev_dir.exists():
            typer.echo(f"Loading Stage {prev_stage} checkpoint from: {prev_dir}")
            load_model_name = str(prev_dir)
        else:
            typer.echo(f"WARNING: Stage {prev_stage} checkpoint not found, loading base model")
            load_model_name = model_name
    else:
        load_model_name = resume if resume else model_name

    t0 = time.time()
    model_obj, tokenizer = FastLanguageModel.from_pretrained(
        model_name=load_model_name,
        max_seq_length=train_cfg["max_seq_length"],
        dtype=None,
        load_in_4bit=model_cfg["model"].get("load_in_4bit", True),
        trust_remote_code=model_cfg["model"].get("trust_remote_code", False),
    )
    typer.echo(f"Model loaded in {time.time() - t0:.1f}s")

    # --- Step 2: Apply LoRA ---
    typer.echo("\n--- Applying LoRA adapters ---")
    model_obj = FastLanguageModel.get_peft_model(
        model_obj,
        r=lora_cfg["r"],
        lora_alpha=lora_cfg["alpha"],
        target_modules=lora_cfg["target_modules"],
        lora_dropout=lora_cfg["dropout"],
        bias=lora_cfg["bias"],
        use_gradient_checkpointing=lora_cfg["use_gradient_checkpointing"],
        random_state=lora_cfg["random_state"],
    )

    trainable = sum(p.numel() for p in model_obj.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model_obj.parameters())
    typer.echo(f"Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)")

    if is_moe:
        typer.echo("MoE: Router is FROZEN (not in target_modules)")
        typer.echo(f"MoE: 8 experts x LoRA adapters on w1/w2/w3 + attention")

    # --- Step 3: Load data ---
    typer.echo("\n--- Loading training data ---")
    staged_data = load_staged_data(stage, base_cfg)

    # Add general data for language preservation (Stages 1 and 2)
    general_data = []
    if stage in (1, 2):
        general_data = load_general_data(base_cfg, stage)

    all_data = staged_data + general_data

    import random as rng_module

    rng_module.seed(seed)
    rng_module.shuffle(all_data)

    typer.echo(f"Total training samples: {len(all_data)}")
    typer.echo(f"  Pipeline staged: {len(staged_data)}")
    typer.echo(f"  General (OpenOrca): {len(general_data)}")

    # Load validation data
    val_key = f"stage{stage}_val"
    val_rel_path = base_cfg["data"].get(val_key)
    val_data = []
    if val_rel_path:
        val_path = PROJECT_ROOT / val_rel_path
        if val_path.exists():
            with open(val_path) as f:
                for line in f:
                    line = line.strip()
                    if line:
                        val_data.append(json.loads(line))
            typer.echo(f"Validation samples: {len(val_data)}")

    from datasets import Dataset

    train_dataset = Dataset.from_list(all_data)
    eval_dataset = Dataset.from_list(val_data) if val_data else None

    if dry_run:
        typer.echo("\n--- Dry run complete ---")
        typer.echo(f"Would train on {len(all_data)} samples")
        if all_data:
            sample = all_data[0]
            typer.echo(f"Sample keys: {list(sample.keys())}")
            if "expert" in sample:
                from collections import Counter
                expert_dist = Counter(s.get("expert", "unknown") for s in all_data)
                typer.echo("Expert distribution:")
                for exp, count in expert_dist.most_common():
                    typer.echo(f"  {exp}: {count} ({100 * count / len(all_data):.1f}%)")
        raise typer.Exit(0)

    # --- Step 4: Configure trainer ---
    typer.echo("\n--- Configuring SFT trainer ---")
    from trl import SFTTrainer
    from transformers import TrainingArguments

    formatting_func = build_formatting_func(model, tokenizer, base_cfg)

    # W&B setup
    report_to = mon_cfg["report_to"]
    if report_to == "wandb":
        try:
            import wandb

            wandb.init(
                project=mon_cfg["wandb_project"],
                name=f"{model_cfg['monitoring']['wandb_run_name']}-stage{stage}",
                tags=model_cfg["monitoring"]["wandb_tags"] + [f"stage{stage}"],
                config={
                    "model": model_name,
                    "stage": stage,
                    "stage_name": stage_cfg["name"],
                    "is_moe": is_moe,
                    "lora_r": lora_cfg["r"],
                    "lora_alpha": lora_cfg["alpha"],
                    "learning_rate": stage_lr,
                    "epochs": stage_epochs,
                    "batch_size": train_cfg["per_device_train_batch_size"],
                    "grad_accum": train_cfg["gradient_accumulation_steps"],
                    "max_seq_length": train_cfg["max_seq_length"],
                    "packing": train_cfg["packing"],
                    "train_samples": len(all_data),
                    "val_samples": len(val_data),
                },
            )
        except Exception:
            typer.echo("WARNING: W&B init failed, falling back to tensorboard")
            report_to = "tensorboard"

    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=stage_epochs,
        per_device_train_batch_size=train_cfg["per_device_train_batch_size"],
        gradient_accumulation_steps=train_cfg["gradient_accumulation_steps"],
        learning_rate=stage_lr,
        lr_scheduler_type=train_cfg["lr_scheduler_type"],
        warmup_ratio=train_cfg["warmup_ratio"],
        optim=train_cfg["optim"],
        weight_decay=train_cfg["weight_decay"],
        max_grad_norm=train_cfg["max_grad_norm"],
        bf16=train_cfg["bf16"],
        fp16=train_cfg["fp16"],
        logging_steps=train_cfg["logging_steps"],
        save_steps=train_cfg["save_steps"],
        save_total_limit=train_cfg["save_total_limit"],
        seed=train_cfg["seed"],
        report_to=report_to,
        evaluation_strategy="steps" if eval_dataset else "no",
        eval_steps=train_cfg["save_steps"] if eval_dataset else None,
        load_best_model_at_end=bool(eval_dataset),
        metric_for_best_model="eval_loss" if eval_dataset else None,
    )

    trainer = SFTTrainer(
        model=model_obj,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        formatting_func=formatting_func,
        max_seq_length=train_cfg["max_seq_length"],
        packing=train_cfg["packing"],
        args=training_args,
    )

    # --- Step 5: Train ---
    typer.echo("\n--- Starting training ---")
    t0 = time.time()
    result = trainer.train(resume_from_checkpoint=resume)
    train_time = time.time() - t0

    # --- Step 6: Save ---
    typer.echo("\n--- Saving model ---")
    trainer.save_model(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))

    # Save training results
    results = {
        "model": model_name,
        "model_key": model,
        "is_moe": is_moe,
        "stage": stage,
        "stage_name": stage_cfg["name"],
        "training_loss": result.training_loss,
        "train_time_seconds": train_time,
        "train_samples": len(all_data),
        "val_samples": len(val_data),
        "epochs": stage_epochs,
        "learning_rate": stage_lr,
        "trainable_params": trainable,
        "total_params": total,
        "lora_r": lora_cfg["r"],
        "lora_alpha": lora_cfg["alpha"],
        "output_dir": str(output_dir),
    }

    results_path = output_dir / "training_results.json"
    results_path.parent.mkdir(parents=True, exist_ok=True)
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)

    # --- Step 7: Report ---
    typer.echo("\n" + "=" * 60)
    typer.echo(f"SFT Stage {stage} Complete — {model_name}")
    typer.echo("=" * 60)
    typer.echo(f"Training loss:   {result.training_loss:.4f}")
    typer.echo(f"Train time:      {train_time / 3600:.1f}h ({train_time:.0f}s)")
    typer.echo(f"Samples:         {len(all_data)} train, {len(val_data)} val")
    typer.echo(f"Output:          {output_dir}")
    typer.echo(f"Results:         {results_path}")

    if report_to == "wandb":
        try:
            import wandb

            wandb.finish()
        except Exception:
            pass

    if stage < 3:
        typer.echo(f"\nNext: python scripts/sft_train.py train --model {model} --stage {stage + 1}")
    else:
        typer.echo(f"\nNext: python scripts/merge_export.py merge --model {model} --stage 3")
        typer.echo(f"Then: python scripts/dpo_train.py train --model {model}")


@app.command()
def info(
    model: str = typer.Option("mixtral-8x7b", help="Model key"),
) -> None:
    """Show training configuration without loading anything."""
    base_cfg, model_cfg = load_configs(model)
    is_moe = model_cfg["model"].get("is_moe", False)

    typer.echo(f"Model: {model_cfg['model']['name']}")
    typer.echo(f"Architecture: {'MoE (8 experts)' if is_moe else 'Dense'}")
    typer.echo(f"LoRA: r={base_cfg['lora']['r']}, alpha={base_cfg['lora']['alpha']}")
    typer.echo(f"4-bit: {model_cfg['model'].get('load_in_4bit', True)}")
    typer.echo(f"Max seq: {base_cfg['training']['max_seq_length']}")
    typer.echo(f"Packing: {base_cfg['training']['packing']}")

    for s in (1, 2, 3):
        stage_key = f"stage{s}"
        if stage_key in base_cfg:
            cfg = base_cfg[stage_key]
            typer.echo(f"\nStage {s} ({cfg['name']}):")
            typer.echo(f"  Target: {cfg.get('target_samples', 'N/A')} samples")
            typer.echo(f"  LR: {cfg.get('learning_rate', base_cfg['training']['learning_rate'])}")
            typer.echo(f"  Epochs: {cfg.get('num_epochs', base_cfg['training']['num_epochs'])}")
            for k, v in cfg["mix"].items():
                typer.echo(f"  {k}: {v:.0%}")

    if "dpo" in base_cfg:
        dpo = base_cfg["dpo"]
        typer.echo(f"\nDPO:")
        typer.echo(f"  LR: {dpo['learning_rate']}, beta: {dpo['beta']}")
        for k, v in dpo["mix"].items():
            typer.echo(f"  {k}: {v:.0%}")

    # Check data availability
    typer.echo("\nData availability:")
    for s in (1, 2, 3):
        key = f"stage{s}_train"
        rel_path = base_cfg["data"].get(key, "")
        path = PROJECT_ROOT / rel_path if rel_path else None
        exists = path.exists() if path else False
        count = 0
        if exists:
            with open(path) as f:
                count = sum(1 for line in f if line.strip())
        status = f"{count} samples" if exists else "NOT FOUND"
        typer.echo(f"  Stage {s}: {status}")


if __name__ == "__main__":
    app()
