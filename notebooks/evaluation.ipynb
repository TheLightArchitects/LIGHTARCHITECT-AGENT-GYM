{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# MCP Agent Evaluation Gym\n",
    "\n",
    "This notebook evaluates three agent types (RandomAgent, RuleBasedAgent, HillClimberAgent)\n",
    "across all loaded scenarios, trains the hill-climber, and compares traces against the\n",
    "production corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import all modules, create the environment, and load scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from mcp_gym.env import make_env\n",
    "from mcp_gym.agents import RandomAgent, RuleBasedAgent, HillClimberAgent\n",
    "from mcp_gym.rewards import MultiDimensionalReward, RewardBreakdown\n",
    "from mcp_gym.scenarios import load_all_scenarios\n",
    "from mcp_gym.trace import TraceLogger, TraceAnalyzer, DEFAULT_CORPUS, EpisodeTrace\n",
    "from mcp_gym.training import train_hill_climber, TrainingConfig, compute_baseline\n",
    "from mcp_gym.types import EpisodeConfig\n",
    "\n",
    "SCENARIO_DIR = Path(\"../scenarios\")\n",
    "SEED = 42\n",
    "\n",
    "# Load all scenarios\n",
    "scenarios = load_all_scenarios(SCENARIO_DIR)\n",
    "print(f\"Loaded {len(scenarios)} scenarios:\")\n",
    "for s in scenarios:\n",
    "    phases = len(s.phases)\n",
    "    print(f\"  - {s.name} ({s.domain}, {phases} phase{'s' if phases != 1 else ''})\")\n",
    "\n",
    "# Get available tools from the environment\n",
    "env = make_env()\n",
    "tools = env.surface.registered_actions()\n",
    "print(f\"\\nRegistered tools ({len(tools)}): {tools[:8]}{'...' if len(tools) > 8 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 2. Run All Agents on All Scenarios\n",
    "\n",
    "Execute each agent type against every scenario and collect rubric scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAMES = [\"random\", \"rule-based\", \"hill-climber\"]\n",
    "\n",
    "\n",
    "def make_agent(name: str, tools: list[str], seed: int = SEED):\n",
    "    \"\"\"Construct an agent by name.\"\"\"\n",
    "    if name == \"random\":\n",
    "        return RandomAgent(tools, seed=seed)\n",
    "    if name == \"rule-based\":\n",
    "        return RuleBasedAgent(tools, seed=seed)\n",
    "    if name == \"hill-climber\":\n",
    "        return HillClimberAgent(tools, seed=seed)\n",
    "    raise ValueError(f\"Unknown agent: {name}\")\n",
    "\n",
    "\n",
    "def run_episode(agent, scenario_def, seed=SEED):\n",
    "    \"\"\"Run one episode, return (reward, breakdown, call_history).\"\"\"\n",
    "    max_steps = max(1, sum(p.max_steps for p in scenario_def.phases))\n",
    "    config = EpisodeConfig(\n",
    "        max_steps=max_steps,\n",
    "        token_budget=scenario_def.token_budget,\n",
    "        seed=seed,\n",
    "    )\n",
    "    env = make_env(config=config)\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    agent.reset()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        action_str = agent.act(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action_str)\n",
    "        total_reward += reward\n",
    "\n",
    "    reward_sys = MultiDimensionalReward()\n",
    "    breakdown = reward_sys.compute(env._call_history, {\n",
    "        \"expected_sequence\": scenario_def.expected_sequence,\n",
    "        \"forbidden_actions\": scenario_def.forbidden_actions,\n",
    "        \"token_budget\": scenario_def.token_budget,\n",
    "        \"tokens_used\": scenario_def.token_budget - env._token_budget,\n",
    "    })\n",
    "    return total_reward, breakdown, env._call_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[agent_name][scenario_name] = RewardBreakdown\n",
    "results: dict[str, dict[str, RewardBreakdown]] = {}\n",
    "\n",
    "for agent_name in AGENT_NAMES:\n",
    "    results[agent_name] = {}\n",
    "    for scenario_def in scenarios:\n",
    "        agent = make_agent(agent_name, tools)\n",
    "        _, breakdown, _ = run_episode(agent, scenario_def)\n",
    "        results[agent_name][scenario_def.name] = breakdown\n",
    "\n",
    "print(f\"Evaluated {len(AGENT_NAMES)} agents x {len(scenarios)} scenarios = {len(AGENT_NAMES) * len(scenarios)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matrix-header",
   "metadata": {},
   "source": [
    "## 3. Comparison Matrix\n",
    "\n",
    "Agent x Scenario total rubric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute column widths\n",
    "name_w = max(len(s.name) for s in scenarios)\n",
    "agent_w = 14\n",
    "\n",
    "# Header\n",
    "header = f\"{'Scenario':<{name_w}}\"\n",
    "for a in AGENT_NAMES:\n",
    "    header += f\"  {a:>{agent_w}}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Rows\n",
    "agent_totals = {a: 0.0 for a in AGENT_NAMES}\n",
    "for s in scenarios:\n",
    "    row = f\"{s.name:<{name_w}}\"\n",
    "    for a in AGENT_NAMES:\n",
    "        score = results[a][s.name].total\n",
    "        agent_totals[a] += score\n",
    "        row += f\"  {score:>{agent_w}.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# Averages\n",
    "print(\"-\" * len(header))\n",
    "avg_row = f\"{'AVERAGE':<{name_w}}\"\n",
    "n = len(scenarios)\n",
    "for a in AGENT_NAMES:\n",
    "    avg = agent_totals[a] / n if n > 0 else 0.0\n",
    "    avg_row += f\"  {avg:>{agent_w}.4f}\"\n",
    "print(avg_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensions-header",
   "metadata": {},
   "source": [
    "### Per-Dimension Averages\n",
    "\n",
    "Average score per reward dimension across all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensions",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [\"judgment\", \"safety\", \"efficiency\", \"context_maintenance\", \"escalation\"]\n",
    "dim_w = 20\n",
    "\n",
    "header = f\"{'Dimension':<{dim_w}}\"\n",
    "for a in AGENT_NAMES:\n",
    "    header += f\"  {a:>{agent_w}}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for dim in dims:\n",
    "    row = f\"{dim:<{dim_w}}\"\n",
    "    for a in AGENT_NAMES:\n",
    "        avg = sum(getattr(results[a][s.name], dim) for s in scenarios) / len(scenarios)\n",
    "        row += f\"  {avg:>{agent_w}.4f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 4. Training Demo\n",
    "\n",
    "Train the hill-climber for 50 episodes and show before/after improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainingConfig(\n",
    "    num_episodes=50,\n",
    "    perturb_magnitude=0.3,\n",
    "    seed=SEED,\n",
    "    scenario_dir=str(SCENARIO_DIR),\n",
    ")\n",
    "\n",
    "train_result = train_hill_climber(train_config)\n",
    "\n",
    "print(f\"Training complete: {train_config.num_episodes} episodes\")\n",
    "print(f\"Best reward:    {train_result.best_reward:+.4f} (episode {train_result.best_episode})\")\n",
    "print(f\"Improvement:    {train_result.improvement:+.4f}\")\n",
    "print()\n",
    "print(\"Initial weights:\")\n",
    "for k, v in sorted(train_result.initial_weights.items()):\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "print()\n",
    "print(\"Final weights:\")\n",
    "for k, v in sorted(train_result.final_weights.items()):\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "print()\n",
    "\n",
    "# Show reward progression (first 10, last 10)\n",
    "rewards = train_result.episode_rewards\n",
    "print(\"Reward progression:\")\n",
    "print(f\"  First 10: {[f'{r:+.3f}' for r in rewards[:10]]}\")\n",
    "print(f\"  Last  10: {[f'{r:+.3f}' for r in rewards[-10:]]}\")\n",
    "print(f\"  Mean (first 10): {sum(rewards[:10]) / min(10, len(rewards)):+.4f}\")\n",
    "print(f\"  Mean (last  10): {sum(rewards[-10:]) / min(10, len(rewards)):+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trace-header",
   "metadata": {},
   "source": [
    "## 5. Trace Analysis\n",
    "\n",
    "Compare agent traces against the DEFAULT_CORPUS of production patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = TraceAnalyzer(DEFAULT_CORPUS)\n",
    "all_traces: list[EpisodeTrace] = []\n",
    "\n",
    "for agent_name in AGENT_NAMES:\n",
    "    agent = make_agent(agent_name, tools)\n",
    "    for scenario_def in scenarios:\n",
    "        logger = TraceLogger()\n",
    "        max_steps = max(1, sum(p.max_steps for p in scenario_def.phases))\n",
    "        config = EpisodeConfig(\n",
    "            max_steps=max_steps,\n",
    "            token_budget=scenario_def.token_budget,\n",
    "            seed=SEED,\n",
    "        )\n",
    "        env = make_env(config=config)\n",
    "        obs, _ = env.reset(seed=SEED)\n",
    "        agent.reset()\n",
    "\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        step_num = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action_str = agent.act(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action_str)\n",
    "            total_reward += reward\n",
    "            step_num += 1\n",
    "\n",
    "            parsed = json.loads(action_str)\n",
    "            logger.log_step(\n",
    "                step=step_num,\n",
    "                server=parsed[\"server\"],\n",
    "                action=parsed[\"action\"],\n",
    "                params=parsed.get(\"params\", {}),\n",
    "                success=info.get(\"success\", False),\n",
    "            )\n",
    "\n",
    "        trace = logger.finish_episode(agent_name, scenario_def.name, total_reward)\n",
    "        all_traces.append(trace)\n",
    "\n",
    "print(f\"Collected {len(all_traces)} traces ({len(AGENT_NAMES)} agents x {len(scenarios)} scenarios)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trace-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = analyzer.compare_traces(all_traces)\n",
    "\n",
    "# Display results\n",
    "agent_w = 14\n",
    "scenario_w = max(len(r[\"scenario_name\"]) for r in comparison)\n",
    "pattern_w = 28\n",
    "\n",
    "header = f\"{'Agent':<{agent_w}}  {'Scenario':<{scenario_w}}  {'Best Pattern':<{pattern_w}}  {'Score':>6}  {'Reward':>8}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for r in comparison[:20]:  # Top 20\n",
    "    pattern_name = r[\"best_pattern\"] or \"(none)\"\n",
    "    print(\n",
    "        f\"{r['agent_name']:<{agent_w}}  \"\n",
    "        f\"{r['scenario_name']:<{scenario_w}}  \"\n",
    "        f\"{pattern_name:<{pattern_w}}  \"\n",
    "        f\"{r['best_score']:>6.3f}  \"\n",
    "        f\"{r['total_reward']:>+8.3f}\"\n",
    "    )\n",
    "\n",
    "if len(comparison) > 20:\n",
    "    print(f\"... and {len(comparison) - 20} more rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Key findings from the evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MCP Agent Evaluation Gym -- Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Best average agent\n",
    "n = len(scenarios)\n",
    "avg_scores = {}\n",
    "for a in AGENT_NAMES:\n",
    "    avg_scores[a] = sum(results[a][s.name].total for s in scenarios) / n\n",
    "\n",
    "best_agent = max(avg_scores, key=avg_scores.get)\n",
    "print(f\"1. Best overall agent: {best_agent} (avg rubric: {avg_scores[best_agent]:.4f})\")\n",
    "for a in AGENT_NAMES:\n",
    "    print(f\"   - {a}: {avg_scores[a]:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Training improvement\n",
    "print(f\"2. Hill-climber training ({train_config.num_episodes} episodes):\")\n",
    "print(f\"   - Improvement: {train_result.improvement:+.4f}\")\n",
    "print(f\"   - Best reward: {train_result.best_reward:+.4f} (episode {train_result.best_episode})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Best trace match\n",
    "if comparison:\n",
    "    best_trace = comparison[0]\n",
    "    print(f\"3. Best corpus match:\")\n",
    "    print(f\"   - Agent: {best_trace['agent_name']}\")\n",
    "    print(f\"   - Scenario: {best_trace['scenario_name']}\")\n",
    "    print(f\"   - Pattern: {best_trace['best_pattern']}\")\n",
    "    print(f\"   - Similarity: {best_trace['best_score']:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Safety analysis\n",
    "print(\"4. Safety scores (avg across all scenarios):\")\n",
    "for a in AGENT_NAMES:\n",
    "    avg_safety = sum(results[a][s.name].safety for s in scenarios) / n\n",
    "    print(f\"   - {a}: {avg_safety:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
